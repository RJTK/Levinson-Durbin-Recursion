\documentclass[12pt]{article}

%% bibliography stuff -- this needs come before the preamble inclusion
\usepackage[backend=bibtex,sorting=none]{biblatex}
\usepackage{enumitem}
\usepackage{etex,etoolbox}
\usepackage{hyperref}
\usepackage{todonotes}
\bibliography{\string~/Documents/academics/global_academics/global_bib.bib}

\input{\string~/Documents/academics/global_academics/latex_preamble}

\title{The Whittle-Levinson-Durbin Recursion}

\author{R. J. Kinnear}

\begin{document}
\maketitle

\abstract{A brief overview of the Levinson-Durbin recursion for
  estimating autoregressive time series models is given.  Whittle's
  generalized (multivariate) version is also expounded upon. We are
essentially summarizing content from
\cite{hayes_statistical_digital_signal_processing},
\cite{lutkepohl2005new}, \cite{whittle_generalized_levinson_durbin}.

It will be seen that the algorithm itself provides deep insights into
the structure of autoregressive time series models, and provides an
indispensable algorithm in practice.}

\tableofcontents
\listoftodos

\section{introduction}
Suppose we have a process $x(t) \in \R^n$ generated by the all-pole model

\begin{equation}
  \sum_{\tau = 0}^p A(\tau)x(t - \tau) = v_t,
\end{equation}

where $B(0) = I_n$ and $v_t$ is a temporally uncorrelated driving
sequence with $\E[v_t] = 0$.  This is closely connected to fitting
$VAR(p)$ time series models

\begin{equation}
  \label{eqn:var}
  \widehat{x}(t) = \sum_{\tau = 1}^p B(\tau)x(t - \tau),
\end{equation}

where one can estimate $A$ and then simply drop $A(0) = I$ and take $B = -A$.

If we observe only $x(t)$, how do we determine $A(\tau)$?  An answer
is provided by the Levinson-Durbin recursion.  This algorithm is
extremely efficient as it allows one to fit a sequence of $VAR(p)$
models for every $p = 1, \ldots, p_{\text{max}}$ all for the cost of
inverting a single toeplitz (or block-toeplitz) matrix.  This implies
that there is effectively no additional cost for performing model
selection (i.e. choosing $p$) over and above what it costs to fit a
single $VAR(p_{\text{max}})$ model.

\subsection{Yule-Walker Equations}
The Levinson-Durbin recursion is essentially an efficient procedure
for solving the Yule-Walker equations in one dimension, and Whittle's
generalization extends to the multivariate case.  The Yule-Walker
equations are simply

\begin{equation}
  \label{eqn:yw}
  \sum_{\tau = 0}^p A(\tau) R(s - \tau) = \delta_s \Sigma_v; s = 0, 1, \ldots, p,
\end{equation}

which describes the relationship between the coefficients $A$ of
Equation \eqref{eqn:var} and the covariance sequence
$R(\tau) = \E[x(t)x(t - \tau)^\T]$.  They can be derived easily by
taking the model \eqref{eqn:var} and multiplying it on the right by
$x(t - \tau)^\T$ and computing the expectation.

There is a close relationship between the Yule-Walker equations and
Toeplitz matrices: If one is to write out equation \ref{eqn:yw} in a
large matrix format, the resulting system is a toeplitz (or
block-toeplitz in the multivariate case) system\footnote{The symbol
  $\otimes$ indicates the Kronecker product}

\begin{equation}
  \mathbf{R} \mathbf{A} = e_1 \otimes \Sigma_v,
\end{equation}

where $\mathbf{A} = [I\ A(1)^\T\ \cdots\ A(p)^\T]^\T$ and $\mathbf{R}$
is a (symmetric) block-toeplitz matrix consisting of
blocks\footnote{One must keep careful track of transposes.}
$[\mathbf{R}]_{s\tau} = R(s - \tau)^\T$, and
$e_1 \otimes \Sigma_v = [\Sigma_v\ 0\ \cdots\ 0]$.  It is critical to
notice that the variables in this equation are $A(1), \ldots, A(p)$
\textit{and} $\Sigma_v$, so it is not written in the usual
``$Ax = b$'' format, but is still a linear equation.  In the
unidimensional case we can write

\begin{equation}
  Ra = \sigma_v e_1,
\end{equation}

where $R$ is a bona-fide toeplitz matrix.

\subsection{Estimating Covariances}
Given a finite sample of data $\{x(t)\}_{t = 1}^T$, we can treat this
as an infinitely extended sequence $\widetilde{x}(t)$ where
$\widetilde{x}(t) = 0$ for $t \le 0$ or $t > T$ (i.e. a rectangularly
windowed sequence) and then estimate the covariance via

\begin{equation}
  \widehat{R}(\tau) = \frac{1}{T}\sum_{t = \tau + 1}^px(t)x(t - \tau)^\T.
\end{equation}

It is critical to use this particular covariance estimator in order to
ensure that $R(\tau)$ is a positive (semi-)definite sequence, that is,
the Toeplitz matrix $\mathbf{R}$ satisfies $\mathbf{R} \succeq 0$.

\section{The Recursions}

\subsection{The Levinson-Durbin Recursion}
We will first write down the Levinson-Durbin recursion, which is the
unidimensional method for solving equation \ref{eqn:yw}.  We will
consider the ``API'' for this Algorithm as taking as input a sequence
of $p + 1$ covariance estimates $\big(r(0), r(1), \ldots, r(p)\big)$ for a
unidimensional ($n = 1$) time series $x(t)$, and returning $p + 1$
variance estimates $\sigma_0^2, \sigma_1^2, \ldots, \sigma_p^2$, as
well as a sequence $\mathbf{b}_0, \mathbf{b}_1, \ldots, \mathbf{b}_p$
where $\mathbf{b}_k \in \R^{k}$ provides coefficients for an $AR(k)$
model of order $k$, where the estimated mean squared error of this
model is given by $\sigma_k^2$

\todo{Double check results for $\sigma_k^2$}
\begin{equation}
  \begin{aligned}
    \widehat{x}(t) &= \sum_{\tau = 0}^k b_k(\tau) x(t - \tau),\\
    \sigma_k^2 &= \frac{1}{T}\sum_{t = 1}^T (x(t) - \widehat{x}(t))^2\\
    &= \E(x(t) - \widehat{x}(t))^2 + O\big(\frac{1}{\sqrt{T}}\big).
  \end{aligned}
\end{equation}

The algorithm is also applicable when $x(t) \in \C^n$, therefore in
Algorithm \ref{alg:levinson_durbin} $*$ indicates complex
conjugate.  It is important to keep in mind that
$r(-\tau) = r(\tau)^*$

\begin{algorithm}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{Initialize}{initialize}
  \DontPrintSemicolon

  \BlankLine
  \caption{Levinson-Durbin Recursion}
  \label{alg:levinson_durbin}
  \Input{Covariance Sequence $r(0), \ldots, r(p)$}
  \Output{$AR$ coefficients $\mathbf{b}_1, \ldots, \mathbf{b}_p$ and
    error estimates $\sigma_0^2, \ldots, \sigma_p^2$}
  \Initialize{
    $a_0(0) = 1$\\
    $\sigma_0^2 = r(0)$\\
  }
  \BlankLine

  \For{$k = 0, \ldots, p - 1$}{
    $\gamma = \sum_{\tau = 0}^ka_k(\tau)r(k - \tau + 1)$\\
    $a_{k + 1}(k + 1) = -\gamma / \sigma_k^2$ \texttt{\# Reflection Coefficient}\\
    $a_{k + 1}(0) = 1$\\
    \For{$\tau = 1, \ldots, k$}{
      $a_{k + 1}(\tau) = a_k(\tau) + a_{k + 1}(k + 1) a_k^*(k - \tau + 1)$ \texttt{\# Copy to next array}\\
    }
    $\sigma_{k + 1}^2 = \sigma_k^2 (1 - |a_{k + 1}(k + 1)|^2)$\\

    \BlankLine
    $\mathbf{b}_{k + 1} = \big(-a_{k + 1}(1), \ldots, -a_{k + 1}(k + 1) \big)$ \texttt{\# Convert to VAR Coefficients}\\

    \BlankLine
    $\mathsf{assert }\sum_{\tau = 0}^{k + 1} a_{k + 1}(\tau)r(s - \tau) = 0;\ \text{for } s = 1, \ldots, k + 1$ \texttt{\# Verify}\\
  }

  \Return{$\big(\mathbf{b}_1, \ldots, \mathbf{b}_p\big)$, $(\sigma_0^2, \ldots, \sigma_p^2)$}
\end{algorithm}

\subsubsection{Properties}
The algorithm runs in $O(p^2)$ time (whereas standard matrix inversion
requires $O(p^3)$ time).  As well there are a number of remarkable
properties associated to Algorithm \ref{alg:levinson_durbin}:

\begin{enumerate}
  \item{$|b_k(k)| \le 1$ if and only if $r(0), \ldots, r(k)$ is positive semi-definite for $k = 1, \ldots, p$}
  \item{$\sigma_k^2 \ge 0$ if and only if $r(0), \ldots, r(k)$ is positive semi-definite for $k = 1, \ldots, p$}
  \item{The $AR(k)$ model obtained from $\mathbf{b}_k$ is stable}
\end{enumerate}

\subsection{Whittle's Generalization}
Whittle \cite{whittle_generalized_levinson_durbin} generalized
Algorithm \ref{alg:levinson_durbin} to the multivariate case.  This
generalization is non-trivial and requires both a \textit{forwards}
set of coefficients $A(\tau)$, but also a \textit{backwards} set of
coefficients $\bar{A}(\tau)$ corresponding to the anti-causal system

\begin{equation}
  \sum_{\tau = 0}^p\bar{A}(\tau)x(t + \tau) = \bar{v}_t.
\end{equation}

The most direct reason that the Levinson-Durbin recursion does not
immediately generalize is simply because matrix multiplication is not
commutative.

The algorithm will consume a sequence of covariance matrices
$R(0), \ldots, R(p)$, and return a sequence
$\mathbf{B}_1, \ldots, \mathbf{B}_p$ of $VAR(k)$ model coefficients,
where $\mathbf{B}_k = \big(B_k(1), \ldots, B_k(k) \big)$ as well as a
sequence $\Sigma_0, \ldots, \Sigma_p$ of error variance matrices where

\todo{Double check results for $\Sigma_k$}
\begin{equation}
  \begin{aligned}
    \widehat{x}(t) &= \sum_{\tau = 1}^p B(\tau)x(t - \tau),\\
    \Sigma_v &= \frac{1}{T} \sum_{t = 1}^T (x(t) - \widehat{x}(t))(x(t) - \widehat{x}(t))^\T\\
    &= \E[(x(t) - \widehat{x}(t))(x(t) - \widehat{x}(t))^\T] + O\big(\frac{1}{\sqrt{T}}\big).
  \end{aligned}
\end{equation}

Keeping in mind that $R(-\tau) = R(\tau)^{\mathsf{H}}$, we have

\begin{algorithm}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKwInOut{Initialize}{initialize}
  \DontPrintSemicolon

  \BlankLine
  \caption{Whittle-Levinson-Durbin Recursion}
  \label{alg:whittle_levinson_durbin}
  \Input{Covariance Sequence $R(0), \ldots, R(p)$}
  \Output{$AR$ coefficients $\mathbf{B}_1, \ldots, \mathbf{B}_p$ and
    error estimates $\Sigma_0, \ldots, \Sigma_p$}
  \Initialize{
    $A_0(0) = I, \bar{A}_0(0) = I$\\
    $\Sigma_0 = R(0), \bar{\Sigma}_0 = R(0)$\\
  }
  \BlankLine

  \For{$k = 0, \ldots, p - 1$}{
    $\Gamma = \sum_{\tau = 0}^kA_k(\tau)R(k - \tau + 1)$\\
    $\bar{\Gamma} = \sum_{\tau = 0}^k\bar{A}_k(\tau)R(\tau - k - 1)$\\

    \BlankLine
    $A_{k + 1}(k + 1) = -\Gamma \bar{\Sigma}_k^{-1}$ \texttt{\# Use 'cholesky' and 'cho\_solve' to invert}\\
    $\bar{A}_{k + 1}(k + 1) = -\bar{\Gamma} \Sigma_k^{-1}$\\

    \BlankLine
    $A_{k + 1}(0) = I, \bar{A}_{k + 1}(0) = I$\\
    \For{$\tau = 1, \ldots, k$}{
      $A_{k + 1}(\tau) = A_k(\tau) + A_{k + 1}(k + 1) \bar{A}_k(k - \tau + 1)$ \texttt{\# Copy to next array}\\
      $\bar{A}_{k + 1}(\tau) = \bar{A}_k(\tau) + \bar{A}_{k + 1}(k + 1) A_k(k - \tau + 1)$\\
    }
    $\Sigma_{k + 1} = \Sigma_k + A_{k + 1}(k + 1) \bar{\Gamma}$ \texttt{\# Update the error variance}\\
    $\bar{\Sigma}_{k + 1} = \bar{\Sigma}_k + \bar{A}_{k + 1}(k + 1) \Gamma$\\
    \BlankLine
    $\mathbf{B}_{k + 1} = \big(-A_{k + 1}(1), \ldots, -A_{k + 1}(k + 1) \big)$ \texttt{\# Convert to VAR coefficients}\\

    \BlankLine
    $\mathsf{assert }\sum_{\tau = 0}^{k + 1} A_{k + 1}(\tau)R(s - \tau) = 0;\ \text{for } s = 1, \ldots, k + 1$ \texttt{\# Verify}\\
    $\mathsf{assert }\sum_{\tau = 0}^{k + 1} \bar{A}_{k + 1}(\tau)R(\tau - s) = 0;\ \text{for } s = 1, \ldots, k + 1$\\

  }

  \Return{$\big(\mathbf{B}_1, \ldots, \mathbf{B}_p\big)$, $(\Sigma_0, \ldots, \Sigma_p)$}
\end{algorithm}

\section{Efficient Model Order Selection}
Given a collection of data, there are a variety of methods to choose
the filter order $p$.  For example, the Bayesian Information Criteria
(BIC) provides a ``meta-optimization'' objective which trades off
between error reduction and model complexity.  Choosing $p$ by the
BIC criterion dictates that we maximize over $p$ the following:

\begin{equation}
  \label{eqn:bic}
    BIC(p) = -(T - p - 1) \ln \tr \Sigma_p - n^2p\ln T.\\
\end{equation}

This can be carried out by a simple direct search on each model order
between $1$ and some prescribed $p_\text{max}$.  In practice it is
sufficient to pick $p_\text{max}$ ad-hoc or via some simple heuristic
(e.g. plotting the sequence $BIC(p)$ over $p$).  Conveniently, the
Levinson-Durbin algorithm provides to us the sequence of error
matrices $\Sigma_p$ all in one go, so choosing $p$ by this criteria
takes time equivalent to simply fitting a single $VAR(p_{\text{max}})$
model.

It must finally be pointed out that this is not a perfect strategy for
model order selection, and is likely only useful when $n$ is quite
small; it is merely one that is efficiently facilitated by the LD
algorithm.  One can in principle make use of the same $BIC$ criterion
to further search for sparse matrices that provide a good fit for the
data, but this becomes a much different problem.
\end{document}
